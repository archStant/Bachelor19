{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#Det har jeg ikke fået til at virke endnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indlæs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = np.load('../data/cullpdb+profile_5926.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuldt datasæt shape:\n",
      "X:  (5926, 700, 22)\n",
      "Y:  (5926, 700, 9)\n",
      "Fuldt datasæt vendt shape:\n",
      "X:  (5926, 22, 700)\n",
      "Y:  (5926, 9, 700)\n",
      "Splittet ud i training og testing:\n",
      "(Train) X:  (5430, 22, 700)\n",
      "(Train) Y:  (5430, 9, 700)\n",
      "(Test)  X:  (255, 22, 700)\n",
      "(Test)  Y:  (255, 9, 700)\n",
      "(Validation)  X:  (236, 22, 700)\n",
      "(Validation)  Y:  (236, 9, 700)\n"
     ]
    }
   ],
   "source": [
    "data = data_raw.reshape((-1,700,57))\n",
    "\n",
    "data.shape\n",
    "x = data[:,:,:22]\n",
    "y = data[:,:,22:31] # (brug 35 hvis du vil have solvent properties med)\n",
    "print('Fuldt datasæt shape:')\n",
    "print('X: ', x.shape)\n",
    "print('Y: ', y.shape)\n",
    "\n",
    "#x = x.reshape(-1,22,700)\n",
    "#y = y.reshape(-1,9,700)\n",
    "x = np.rot90(x, axes=(1,2))\n",
    "y = np.rot90(y, axes=(1,2))\n",
    "\n",
    "x = np.flip(x, 1)\n",
    "y = np.flip(y, 1)\n",
    "\n",
    "print('Fuldt datasæt vendt shape:')\n",
    "print('X: ', x.shape)\n",
    "print('Y: ', y.shape)\n",
    "\n",
    "x_train = x[:5430]\n",
    "y_train = y[:5430]\n",
    "\n",
    "x_test = x[5435:5690]\n",
    "y_test = y[5435:5690]\n",
    "\n",
    "x_validation = x[5690:5926]\n",
    "y_validation = y[5690:5926]\n",
    "\n",
    "print('Splittet ud i training og testing:')\n",
    "print('(Train) X: ', x_train.shape)\n",
    "print('(Train) Y: ', y_train.shape)\n",
    "print('(Test)  X: ', x_test.shape)\n",
    "print('(Test)  Y: ', y_test.shape)\n",
    "print('(Validation)  X: ', x_validation.shape)\n",
    "print('(Validation)  Y: ', y_validation.shape)\n",
    "\n",
    "torch_X_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "torch_Y_train = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "torch_X_test  = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "torch_Y_test  = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
    "torch_X_validation  = torch.from_numpy(x_validation).type(torch.FloatTensor)\n",
    "torch_Y_validation  = torch.from_numpy(y_validation).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sæt data sammen i DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "BATCH_SIZE = 250\n",
    "\n",
    "train = torch.utils.data.TensorDataset(torch_X_train, torch_Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definér modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (700, 22)\n",
    "            nn.Conv1d(\n",
    "                in_channels=22,            # input height\n",
    "                out_channels=30,           # n_filters / feature maps\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (300, 22)\n",
    "            nn.ReLU(),                      # activation\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (700, 22)\n",
    "            nn.Conv1d(\n",
    "                in_channels=30,            # input height\n",
    "                out_channels=20,           # n_filters / feature maps\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (300, 22)\n",
    "            nn.ReLU(),                      # activation\n",
    "        )\n",
    "        self.out = nn.Sequential(         # input shape (700, 22)\n",
    "            nn.Conv1d(\n",
    "                in_channels=20,            # input height\n",
    "                out_channels=9,           # n_filters / feature maps\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (300, 22)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiér modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(22, 30, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(30, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Conv1d(20, 9, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "    (2): Softmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn) # net architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5               # train the training data n times, to save time, we just train 1 epoch\n",
    "LR = 0.005              # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiér loss-funktioner og optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "loss_func = nn.BCELoss()  #Binary Cross Entropy Loss\n",
    "validation_loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Træn modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Det her virker heller ikke endnu \n",
    "# - det skal bruges til at omforme vores forudsigelser til at kunne sammenligne det med vores valideringssæt\n",
    "def RemakeOneHot(a):\n",
    "    #a = np.array([[0, 1], [2, 3], [4, 5], [6, 7], [9, 8]])\n",
    "    b = np.zeros_like(a)\n",
    "    #return (a == a.max(axis=1)[:,None]).astype(int)\n",
    "    return (b[np.arange(len(a)), a.argmax(1)] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "tensor(0.3351, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3319, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3298, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Epoch:  2\n",
      "tensor(0.3293, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2050, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0799, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Epoch:  3\n",
      "tensor(0.0798, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0781, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0777, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Epoch:  4\n",
      "tensor(0.0774, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0771, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0768, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Epoch:  5\n",
      "tensor(0.0767, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0763, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0757, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ', epoch+1)\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):\n",
    "\n",
    "        output = cnn(b_x)[0]               # cnn output\n",
    "        #print(output.shape)\n",
    "        #print(b_y.shape)\n",
    "        #print(output[0,:,0])\n",
    "        #print(b_y[0,:,0])\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        #print(loss)\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            #print(loss)\n",
    "            test_output, last_layer = cnn(torch_X_validation)\n",
    "            #print(test_output.shape)\n",
    "            vloss = validation_loss_func(test_output, torch_Y_validation)\n",
    "            print(vloss)\n",
    "#            print(RemakeOneHot(test_output[0].detach().numpy()))\n",
    "            #accuracy = float((test_output.detach().numpy() == y_validation.data).astype(int).sum()) / float(y_validation.size(0))\n",
    "            #break\n",
    "            #print(loss)\n",
    "            #print(pred_y[0])\n",
    "            #accuracy = float((pred_y == y_test.data).astype(int).sum()) / float(test_y.size(0))\n",
    "            #print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight [10, 22, 5], but got 2-dimensional input of size [700, 22] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-512a5ee696cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# gives batch data, normalize x when iterate train_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mb_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_Y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[0]               # cnn output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# clear gradients for this training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-4af033f9661f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 187\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight [10, 22, 5], but got 2-dimensional input of size [700, 22] instead"
     ]
    }
   ],
   "source": [
    "# Den oprindelige kode jeg stjal - bare som backup\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step, b_x in enumerate(torch_X_train):   # gives batch data, normalize x when iterate train_loader\n",
    "        b_y = torch_Y_train[step]\n",
    "        output = cnn(b_x)#[0]               # cnn output\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            test_output, last_layer = cnn(torch_X_test)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float((pred_y == torch_Y_test.data.numpy()).astype(int).sum()) / float(torch_Y_test.size(0))\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
