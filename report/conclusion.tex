\section{Discussion}
While achieving somewhat good results (genskriv), a few aspects of our decision-making immidiately stand out as possible improvements on our model for potential future iterations. While we did optimize a series of hyperparameters, others were either decided upon early in the process (usually out of practical reasons) or left at library defaults.
\subsection{Optimizing the optimizer}
Among these hyperparameters are the three parameters for the Adam optimization algorithm, apart from the initial learning rate, the $\beta_1, \beta_2$ and $\epsilon$ parameters. While the $\epsilon$ parameter shuold probably not be altered much, the amount of "stuttering" and noise in the accuracy of the models, especially during the end of training, might indicate that performance could be won by tweaking the first two, as they control how quickly the learning rate falls over time.
\subsection{Iterating over data}
One early decision was that of using a batch-size of 4 proteins. This had the consequence that the model was optimized 1355 times during each epoch. By reducing the batch-size, this number could be expanded, allowing both for more training steps and more specifik training of the model. Implementing artificial neural networks with a batch size of 1 (i.e. a general stochastic network) is generally regarded as a favourable method as compared to larger batch sizes.\\
During our project we decided against using this strategy on the grounds of having time and flexibility to do a series of different training runs. This was motivated by the hardware we had available, which was not machine-learning optimized systems but consumer PCs. Possible improvements to the final predictive capabilities of our models could possible be acheived by training on hardware that allowed for smaller batch sizes.\\
Another aspect of the management of data in the training situation that could be optimized is the separation of data into training, test and validation sets. While we opted to follow the recommendations provided in the ReadMe file shipped along side our data, other approaches might prove beneficial to the accuracy. One such approach is the employment of k-fold cross-validation, a method in which the subsets of data chosen to serve as validation and training  are shifted between epochs, such that all proteins in the data set get to serve as both training and validation data.\\
Seing how, especially in the case of the solvent accessibility properties, the accuracy on the validation and test sets started diverging strongly after a few epochs, it is not unlikely that implementing k-fold cross-validation in the model could prove a benefit in future improvement.


\section{Conclusion}
The aim this paper was to put to test the question of whether applying multi-task learning to a convolutional neural network could help improve its accuracy in regards to predicting protein secondary structures.\\
To do this we built first a single-task model predicting Q8 secondary structures from amino acid recidues as well as the sequence profiling, as well as a similar model for predicting relative and absolute solvent accessibility.\\
We then built another model combining these two elements in a hard parameter-sharing multi-task learning neural network, and optimized this with regards to hyperparameters number of layers, learning rate, layer depth and kernel size.\\
Training these three models with the optimized parameters showed that while the single-task model was slightly better at predicting solvent accessibilities, the multi-task model did indeed outperform the single-task when it came to predicting secondary structures.\\
At the offset of this project we used articles by \citeauthor{qi-et-al-2012}, \citeauthor{zhou-and-troyanskaya-2014} as well as \citeauthor{wang-et-al-2016}, noting that the latter of these had also built a convolutional neural network (DeepCNF-SS) with the same goals, reaching a Q8 accuracy of 75.2\% on the same data set as we have used, while we with our model only achieved 71.24\% on our single-task and 71.81\% on our multi-task models. Differences between DeepCNF-SS and our single-task model include both use use of regularization and the use of a Conditional Random Field as a precursive layer. \\
Since our results showed an increase in accuracy when expanding the scope of the model to multi-task learning, one can speculate if a similar increase in performance could be expected if one was to implement multi-task learning into Wang et al.'s superior model.\\
If one was to produce a follow-up to this paper, the Q8 accuracy of our model could potentially be improved in a series of ways. On a lower level, attempts could be made at implementing regularization or adding noise, such as applyong a gaussian filter on the intermediate layers (which has shown to be effective by \citeauthor{zhou-and-troyanskaya-2014}).\\
Alternatively, another approach to improving our model could be that of rethinking its structure. This could be done either by adding fully connected layers in the end for predicting solvent accessibility features or possibly by implementing the multi-task learning aspect in a soft parameter-sharing fashion.


Skriv ogs√• noget om K cross validation af dataen.