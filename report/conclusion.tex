\section{Discussion}
While achieving somewhat good results (genskriv), a few aspects of our decision-making immidiately stand out as possible improvements on our model for potential future iterations. While we did optimize a series of hyperparameters, others were either decided upon early in the process (usually out of practical reasons) or left at library defaults.
\subsection{Optimizing the optimizer}
Among these hyperparameters are the three parameters for the Adam optimization algorithm, apart from the initial learning rate, the $\beta_1, \beta_2$ and $\epsilon$ parameters. While the $\epsilon$ parameter shuold probably not be altered much, the amount of "stuttering" and noise in the accuracy of the models, especially during the end of training, might indicate that performance could be won by tweaking the first two, as they control how quickly the learning rate falls over time.

\subsection{Iterating over data}
One early decision was that of using a batch-size of 4 proteins. This had the consequence that the model was optimized 1355 times during each epoch. By reducing the batch-size, this number could be expanded, allowing both for more training steps and more specifik training of the model. Implementing artificial neural networks with a batch size of 1 (i.e. a general stochastic network) is generally regarded as having som favourable outcomes for certain problems compared to larger batch sizes. For instance the high frequency the model is updated with (once for every batch), can lead to faster learning. Furthermore, it tends to be more noisy while updating, due to the high frequency, which in turn can improve the models possiblity of avoiding local extremas, and thereby to early convergence.\\
As a contrast, using batch gradient is in general seen as more computational efficient, and the decrease in the update frequency has the effect of af more regular and less noisy update of the model, which sometimes leads to a more stable convergence. There is some problems using batch gradient, among them being risk of to early convergence, due to local extremas as well as slower training speeds. The final challenge using batch gradient, is also the one for why we never had the chance to try and compare the results with the way we implemented our network. Namely that it is very memory intensive, as it expects the entire dataset to be in memory and available to the learning algorithm. This combined with the fact we used the memory on the GPU to convolve, means we ran out of memory, and couldn't actually use this method. In between is mini-batch as mentioned earlier. This often end up being, if not the recommended way to calcualte gradient, then at least often used. This is due to taking a lot of the pros and cons of stochastic and batch respectively and balance them out to the model in use. When doing this, batch size ends up being a hyperparameter that can be tweaked. Lower batch size means faster learning but over longer training time, and the lower the batch size, the less of a risk of early convergence. On the other hand, to small batchsize result in less efficient computations, but also less memory intensity. During our project we decided against using the strategy of using stochastic, on the grounds of having time and flexibility to do a series of different training runs. On the other hand, batch gradient were not possible due to the hardware we had available, which was not machine-learning optimized systems but consumer PCs. The same thing also put a dampner on our possibilities for using larger batch sizes in the mini-batch gradient we ended up using. Possible improvements to the final predictive capabilities of our models could possible be acheived by training on hardware that allowed for more freedom in piciking batch size, so it could be tested in a broader range, from stochastic to batchgradient.\\

Another aspect of the management of data in the training situation that could be optimized is the separation of data into training, test and validation sets. While we opted to follow the recommendations provided in the ReadMe file shipped along side our data, other approaches might prove beneficial to the accuracy. As mentioned earlier, when having split of the test set, the remaining data is often split 80-20 into training and validation sets. Our reasoning for why we think that is not what was recommended by the creators of the dataset we have used, is that it is a rather small dataset, in terms of machine learning. As such, they have made a decision on how much trainig data would be nice to keep. An approach we could have used that might have improven accuracy is therefore the employment of k-fold cross-validation. This is a method in which the subsets of data chosen to serve as validation and training are shifted between runs, such that all proteins in the data set get to serve as both training and validation data. A normal choice is to pick k to be 10, and do 10-fold cross validation. In this case, the data is divided into 10 equal sized chunks, and training is then done with 9 of them, the last is used as validation. Afterwards everything is shifted, so it is a new chunk that is left out as validation, until all sets have been used as validation once. Afterwards you average the accuracy from all 10 validations, and thereby the perception of the accuracy will be more accurate.\\ Seing how, especially in the case of the solvent accessibility properties, the accuracy on the validation and test sets started diverging strongly after a few epochs, it is not unlikely that implementing k-fold cross-validation in the model could prove a benefit in future improvement.


\section{Conclusion}
The aim this paper was to put to test the question of whether applying multi-task learning to a convolutional neural network could help improve its accuracy in regards to predicting protein secondary structures.\\
To do this we built first a single-task model predicting Q8 secondary structures from amino acid recidues as well as the sequence profiling, as well as a similar model for predicting relative and absolute solvent accessibility.\\
We then built another model combining these two elements in a hard parameter-sharing multi-task learning neural network, and optimized this with regards to hyperparameters number of layers, learning rate, layer depth and kernel size.\\
Training these three models with the optimized parameters showed that while the single-task model was slightly better at predicting solvent accessibilities, the multi-task model did indeed outperform the single-task when it came to predicting secondary structures.\\
At the offset of this project we used articles by \citeauthor{qi-et-al-2012}, \citeauthor{zhou-and-troyanskaya-2014} as well as \citeauthor{wang-et-al-2016}, noting that the latter of these had also built a convolutional neural network (DeepCNF-SS) with the same goals, reaching a Q8 accuracy of 75.2\% on the same data set as we have used, while we with our model only achieved 71.24\% on our single-task and 71.81\% on our multi-task models. Differences between DeepCNF-SS and our single-task model include both use use of regularization and the use of a Conditional Random Field as a precursive layer. \\
Since our results showed an increase in accuracy when expanding the scope of the model to multi-task learning, one can speculate if a similar increase in performance could be expected if one was to implement multi-task learning into Wang et al.'s superior model.\\
If one was to produce a follow-up to this paper, the Q8 accuracy of our model could potentially be improved in a series of ways. On a lower level, attempts could be made at implementing regularization or adding noise, such as applyong a gaussian filter on the intermediate layers (which has shown to be effective by \citeauthor{zhou-and-troyanskaya-2014}).\\
Alternatively, another approach to improving our model could be that of rethinking its structure. This could be done either by adding fully connected layers in the end for predicting solvent accessibility features or possibly by implementing the multi-task learning aspect in a soft parameter-sharing fashion.


Skriv ogs√• noget om K cross validation af dataen.