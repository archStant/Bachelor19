\section{Introduction}
\subsection{Motivation}
Neurale netvÃ¦rk er rigtigt seje \cite{bishop2006}
\subsection{Proteins}
Proteins are large biomolecules that has a wide range of functions in organisms. Those functions includes giving structure to cells, being part of DNA replication and responding to stimuli. KILDEKILDEKILDE!!. They consist of a chain of amino acids, where it is the sequence of the amino acids that determine the protein. The linear sequence of amino acids in a protein is also known as a proteins primary structure. 

\subsubsection{Secondary Structures}
The secondary structure of a protein largely determined by it's primary structure. Among the many reasons for predicting the secondary structure, using it as a stepping stone for trying to find the three-dimensional structure of a protein, which is normally difficult to assess. The secondary structure itself is the three-dimensional form of local segments of the protein. [Qi, Oja, Weston \& Noble, p. 2].
The secondary structure is normally divided into either 3 or 8 groups in the literature often named Q3 and Q8. Using the DSSP alphabet for giving the groups letters, the groups of Q3 are helix(H), sheets(B) and coils(C).KILDE KILDE
When predicting Q3 secondary structures current state of the art has it's accuracy percentage in the mid eighties. This was for a long time the secondary structure prediction that was done, but due to advances in technology and research, the harder but more detailed Q8 predictions are gaining more attention. According to the 2 articles used as a starting point for this thesis, which will be further showed later, the accuracy for current state of the art when predicting Q8 secondary structure is around 72-74\%(KILDKILDEKILDE)
Again using the DSSP alphabet for giving letters, the categories of Q8 are: alpha Helix(H), residue in isolated beta bridge(B), extended strand(E) 3-helix(G), 5-helix(I), hydrogen bonded turn(T), bent(S), loop(L). Those are the three dimensional forms a protein can take in a local segment.  


\subsubsection{Solvent accessible surface area}
Another feature of a protein we are looking into, is each amino acids solvency. The solvency is a measure of whether or not it can be absolved by water. This is determined by whether or not the acid is deep inside the protein or close to the surface of the protein. This feature is another thing we will try to predict using our neural network. 


\subsection{Neural networks}
The common perception is that humans and animals process information, i.e. transform perceptional 
stimuli and physiological conditions into behaviour, by using their brains. This is imprecise 
however, as the brain as such is only one functioning part of what is called the \textit{nervous 
system}, which is in turn responsible for the internal workings of human and animal behaviour.

This nervous system is an abstaction over a number of \textit{neurons} interconnected by 
synapses. Neurons in turn are so-called electrically exitable cells. In a gross over-simplification, this can be translated into the case that each neuron can have different internal states, depending on the internal states of the neurons it is connected to, thus forming a \textit{neural network}.

%\begin{figure}[h]
%  \centering
%  \frame{\includegraphics[width=0.8\linewidth]{images/Gyrus_Dentatus_40x}}
%  \caption{Neurons in the dentate gyrus of an epilepsy patient.}
%\end{figure}

\begin{Figure}
 \centering
 \includegraphics[width=0.8\linewidth]{images/Gyrus_Dentatus_40x}
 \captionof{figure}{Neurons in the dentate gyrus of an epilepsy patient.}
\end{Figure}

While obviously interesting within the fields of biology or psychology, this structure has shown 
to be enourmously interesting in the field of computation, as one can in fact model this very 
thing and use it to make predictions based on prior observations, for example in regards to the 
aforementioned structures of proteins folding.

Analogously, or rather, digitally, we can use this model to construct an \textit{artificial 
neural network}.
These set themselves apart from biological neural networks in a couple of ways: most urgently in that while neurons in biological neural networks are connected to each other via synapses, so that each neuron has an either inhibitory or activating effect on whether or not a connected neuron 'fires', in artificial neural networks neurons are connected by \textit{edges}, each with an associated \textit{weight}, allowing the artificial networks to leave the discrete domain and enter the continuous.

\subsubsection{Basics}
An artificial neural network is a specific instantiation of a \textit{multi-layer perceptron}.
A multi-layer perceptron in turn is a system of nodes arranged into layers and each receiving their value from the value of the nodes in the layer before them (adjusted by some weight) in combination with a bias (a scalar) and an activation function of some sort. \\
In this way the first layer will be called the \textit{input layer}, the last layer the \textit{output layer} and all layers in between \textit{hidden layers}. \\
In the case of neural networks, the nodes in the system are referred to as neurons.
Thus, if one was to describe it more precisely, the value $z_i$ of neuron layer \textit{i} with \textit{d} number of neurons in it, given the matrix of sets of weights \textit{w} (the biases being the very first row) and the activation function $h()$ can be expressed as follows:
\[
z_i = h\left(\sum_{j=0}^d w_{ij}\cdot z_{i-1} + w_{i0}\right)
\]
Such a model is useful for calculating continuous values as well as for classification purposes. A common example of the latter is the MNIST data set of hand-written digits in 28x28 pixel greyscale images. In this case the light intensity of the 784 ($28^2$) pixels could fittingly be the input layer, while a layer consisting of 10 nodes (corresponding to digits 0 through 9) could be the output layer, such that the system could be used to predict which digit is written in a given image.
%\begin{figure}[h]
%  \centering
%  \frame{\includegraphics[width=0.7\linewidth]{images/mnist}}
%  \caption{Handwritten digits in the MNIST data set.}
%\end{figure}

\begin{Figure}
 \centering
 \includegraphics[width=0.7\linewidth]{images/mnist}
 \captionof{figure}{Handwritten digits in the MNIST data set.}
\end{Figure}

A common implementation of a neural network to perform this task is to have a single hidden layer of 800 nodes, however the most successful implementations in regards to the MNIST data set have been made with \textit{convolutional neural networks}, but we shall return to these later.

\subsubsection{Training}
Simply having defined the system of the neural network as above (denoted $f_{NN}$) naturally does not give us a reliable method for classifying, as the accuracy of the prediction inherently depends on the correctness of the weights and biases of the system (denoted $\Theta$), which must be initialized with random values. \\
Enter a rather nifty idea named \textit{backpropagation}. Having a set of predicted values ($\hat{y}$) for a data set (\textit{x}) along with the observed correct values (\textit{y}), one can establish a \textit{loss} by applying a loss function (that we shall elaborate on later), but which can be as straightforward as a root mean square error. We shall denote this loss function $l(\hat{y},y)$, so that the  total loss of the system when applied to data set \textit{x} is $l(f_{NN}(\Theta, x), \hat{y})$.\\
Now backpropagation refers to the practice of essentially taking the derivative of the loss function in regards to the matrix of weights and biases, thereby calculating a \textit{gradient} $\nabla{l(f_{NN}(\Theta, x), \hat{y})}$ of these values with respect to the accuracy of the system, and then adjusting the weights and biases by the gradient multiplied by a learning rate scalar.\\
In laymans terms, by doing this one gets a matrix of 'change your weights by this and that much in this and that direction to increase prediction accuracy'.
%
%\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=0.4cm,y=0.4cm]
%\clip(-3.84,0.01) rectangle (18.8,18.55);
%\draw [line width=1pt,fill=ffqqqq,fill opacity=1] (2,7) circle (0.4cm);
%\draw [line width=1pt,fill=ffqqqq,fill opacity=1] (2,10) circle (0.4cm);
%\draw [line width=1pt,fill=qqffqq,fill opacity=1] (7,4) circle (0.4cm);
%\draw [line width=1pt,fill=qqffqq,fill opacity=1] (7,7) circle (0.4cm);
%\draw [line width=1pt,fill=qqffqq,fill opacity=1] (7,10) circle (0.4cm);
%\draw [line width=1pt,fill=qqffqq,fill opacity=1] (7,13) circle (0.4cm);
%\draw [line width=1pt,fill=qqttcc,fill opacity=1] (12,10) circle (0.4cm);
%\draw [line width=1pt,fill=qqttcc,fill opacity=1] (12,7) circle (0.4cm);
%\draw [line width=1pt] (3,10)-- (6,13);
%\draw [line width=1pt] (3,10)-- (6,10);
%\draw [line width=1pt] (3,10)-- (6,7);
%\draw [line width=1pt] (3,10)-- (6,4);
%\draw [line width=1pt] (3,7)-- (6,13);
%\draw [line width=1pt] (3,7)-- (6,10);
%\draw [line width=1pt] (3,7)-- (6,7);
%\draw [line width=1pt] (3,7)-- (6,4);
%\draw [line width=1pt] (8,13)-- (11,10);
%\draw [line width=1pt] (8,10)-- (11,10);
%\draw [line width=1pt] (8,7)-- (11,10);
%\draw [line width=1pt] (8,4)-- (11,10);
%\draw [line width=1pt] (8,4)-- (11,7);
%\draw [line width=1pt] (11,7)-- (8,7);
%\draw [line width=1pt] (8,10)-- (11,7);
%\draw [line width=1pt] (11,7)-- (8,13);
%\end{tikzpicture}

\subsubsection{Convolutional neural networks}
Before talking about convolutional neural networks, we have to specify what a convolution is. Again the MNIST data set serves as an obvious example.\\
One can understand each of the images in the data set as a matrix of $26\times26$ values in the range of 0 - 255. The formal definition of a convolution is \emph{TODO}, however the more practical explanation is that it is the resulting matrix of the product of a matrix and a filter. For example if one has the matrix $a$ and the filter $f$:
\[
a = \begin{bmatrix}
       2 & 3 & 4           \\[0.3em]
       1 & 2 & 3 \\[0.3em]
       0 & 1 & 2
     \end{bmatrix}
\hspace*{1cm}
f = \begin{bmatrix}
       0 & 1 & 0           \\[0.3em]
       1 & 1 & 1 \\[0.3em]
       0 & 1 & 0
     \end{bmatrix}     
\]
the result of convolving $a$ over $f$ would be:
\subsubsection{Multitask learning}
The concept of multitask learning with neural networks is both simple and ingenious. 


\subsection{Prior research in this field}
Skrive noget om den artikel vi tager udgangspunkt i.











