\section{Introduction}
Before presenting the motivation for this paper and the concepts used throughout it, we would like to elaborate on the purpose of the thesis. In our initial contract we have the following description of the purpose of the paper in Danish:\\\\
\textit{The purpose of this project is to learn how neural networks are based theoretically, to learn how they can be applied to solve specific tasks (in this case to predict protein properties based on their composition of amino acids) as well as achieving a basic understanding of these features. \\\\
We expect the process to have the following parts:\\
- Implementing a rudimentary neural network without use of newer tools, with the motivation to be able to explain the theoretical background for it.\\
- A real implementation of a artificial neural network based on established frameworks, such as PyTorch, with the motivation to be able to predict protein secondary structures, based on sequences of amino acids. \\
- A similar implementation with the motivation of being able to predict Solvent Accessible Surface Area based on amino acid sequences. \\
- A comparison of result from the two above mentioned models, contrasted with a model that uses multi-task learning to predict both of those properties.\\ }\\
Using this description as a guide through our work, we started with a look at a rudimentary implementation of a neural network. As a starting point for this, we were given a Jupyter-notebook with some introductory explanations and exercises to do developed by Denny \citeauthor{britz}.\\
After completing the exercises and tweaking the code to get a better understanding of the methods and concepts, we afterwards implemented the same neural network using the library PyTorch on top of Python. 
This gave us a basic understanding of where to start coding our real neural networks. As it was used as a practice exercise for us to get started and achieve a basic level of understanding, we have not included the code or exercises in the paper.

\subsection{Motivation}
In computer science today, one of the areas of research that is publicly debated and covered by media and politicians alike, is the subject of Artificial Intelligence. Within this matter concepts like deep learning, machine learning and neural networks are showing up repeatedly as both publicly used buzzwords and central areas for future research and use.\\
This paper's purpose is threefold. The first is to give an introductory understanding of the topic of machine learning and more specifically neural networks. This includes a basic understanding of the theoretical basis of neural networks. \\
The second purpose is to widen this understanding of neural networks by implementing one to handle a specific task. This task is to predict secondary structure of proteins based on their amino acid composition. Lastly the purpose is to give a rudimentary understanding of what proteins and their composites are, to give context to the use of the neural network.\\
The motivation for choosing protein secondary structure prediction for the neural networks also has several reasons. \\
Protein secondary structure prediction has many real world applications and is an important branch of bioinformatics today, as it is used in both medicine, biotechnology and research into diseases such as Alzheimer's. \citep[p. 113]{BrandenCarl1991Itps}\\
Improving predictive capabilities in regards to protein secondary structure is, at the time of writing, a very active area of research. As such it gives a contemporary and relevant context for this paper as an interesting introduction to neural networks and their application.   
A common example when learning about neural networks and implementing them is to identify a handwritten number between 0 and 9. This neural network is often trained on the MNIST dataset of handwritten numbers. \\
We will use the MNIST dataset of numbers to explain some of the basics of neural networks. 


\subsection{Proteins}
Proteins are large bio-molecules that serve a wide range of functions in organisms. Those functions includes giving structure to cells, being part of DNA replication and responding to stimuli. They consist of chains of amino acids, the sequence of which determine the type of the protein. The linear sequence of amino acids in a protein is also known as a protein's primary structure. \cite{BrandenCarl1991Itps}

\subsubsection{Secondary Structures}
The secondary structures are the three-dimensional forms a protein can take in a local segment and are largely determined by its primary structure. There exists many reasons for research into predicting the secondary structure. One of these is using it as a stepping stone for trying to find the three-dimensional tertiary structure of a protein (which is normally difficult to assess). The secondary structure itself is the three-dimensional form of local segments of the protein \citep[p.~2]{qi-et-al-2012}.
The secondary structure is divided into groups of either 3 or 8 types of structure, named Q3 and Q8 \citep{zhou-and-troyanskaya-2014}. 
Using the DSSP alphabet \citep{kabsch-and-sander-1983} 
for giving the groups letters, the categories of Q8 are as shown in the Q8 table \citep{qi-et-al-2012}.
\begin{table}[H]
\caption{Q8 structure overview}
\centering
\begin{tabular}{l|l}
\hline 
Letter	& Secondary Structure 			\\ \hline
H	& Alpha-helix						\\
B	& residue in isolated beta bridge	\\
E	& extended strand					\\
G	& 3-helix							\\
I	& 5-helix							\\
T	& hydrogen bonded turn				\\
S	& bend								\\
L	& loop								\\
\end{tabular}
\end{table}
\noindent The accuracy for current state of the art when predicting Q8 secondary structure is around 72-76\% \citep{zhou-and-troyanskaya-2014, qi-et-al-2012, wang-et-al-2016}. \\
When predicting Q3 secondary structures the accuracy percentage of the current state of the art lies in the mid eighties. This was for a long time the main focus of research in secondary structure prediction, but due to recent advances in technology and research, the harder but more specific Q8 predictions are gaining more attention. 
Again using the DSSP alphabet for giving letters, the groups of Q3 are as in the Q3 table \citep{qi-et-al-2012}.

\begin{table}[H]
\caption{Q3 structure overview}
\centering
\begin{tabular}{l|l}
\hline
Letter		& Q8 structures in group	\\ \hline
H	(helix)		& H, G						\\
B	(sheet)		& B, E						\\
C (coil)	    & I, S, T, L				\\
\end{tabular}
\end{table}

\end{multicols}
\begin{Figure}
 \centering
 \includegraphics[width=0.8\linewidth]{images/protein_structure}
 \captionsetup{width=0.8\linewidth, font=small}
 \captionof{figure}{Levels of protein structure. \citep{BrandenCarl1991Itps}}
\end{Figure}
\begin{multicols}{2}

\subsubsection{Solvent accessible surface area}
Another feature of proteins this paper shall examine, is amino acid solvent accessible surface area. This solvency is a measure of whether or not it can be absolved by water. Among other things this is determined by whether or not the acid is deep inside the protein or close to the surface of the protein. \citep[p. 3]{qi-et-al-2012}\\
This shall serve as the secondary task for the implementation of a multi-task learning model.

\subsection{Neural networks}
The common perception is that humans and animals process information, i.e. transform perceptional 
stimuli and physiological conditions into behavior, by using their brains. This is imprecise 
however, as the brain as such is only one functioning part of what is called the \textit{nervous 
system}, which is in turn responsible for the internal workings of human and animal behavior.\\
This nervous system is an abstraction over a number of \textit{neurons} interconnected by 
synapses. Neurons in turn are so-called electrically excitable cells. In a gross over-simplification, this can be explained as such: each neuron can have different internal states, depending on the internal states of the neurons it is connected to, thus forming a \textit{neural network}.

%\begin{figure}[h]
%  \centering
%  \frame{\includegraphics[width=0.8\linewidth]{images/Gyrus_Dentatus_40x}}
%  \caption{Neurons in the dentate gyrus of an epilepsy patient.}
%\end{figure}

\noindent While obviously interesting within the fields of biology or psychology, this structure has shown 
to be useful in the field of computation, as one can in fact model this very 
thing and use it to make predictions based on prior observations, for example in regards to the 
aforementioned structures of proteins folding.\\
\begin{Figure}
 \centering
 \includegraphics[width=0.8\linewidth]{images/Gyrus_Dentatus_40x}
 \captionsetup{width=0.8\linewidth, font=small}
 \captionof{figure}{Neurons in the dentate gyrus of an epilepsy patient.}
\end{Figure}
\noindent Analogously, or rather digitally, we can use this model to construct an \textit{artificial 
neural network}.
These set themselves apart from biological neural networks in a couple of ways. \\
In biological neural networks neurons are connected to each other via synapses. In this way, each neuron has an either inhibitory or activating effect on whether or not a connected neuron 'fires'.\\
In contrast to this, artificial neural networks' neurons are connected by \textit{edges}, each with an associated (real number) \textit{weight}, allowing the artificial networks to leave the discrete domain and enter the continuous.

\subsubsection{Basics}
An artificial neural network is a specific instantiation of a \textit{multi-layer perceptron}.
A multi-layer perceptron in turn is a system of nodes arranged into layers and each receiving their value from the value of the nodes in the layer before them (adjusted by some weight) in combination with a bias (a scalar) and an activation function of some sort. \\
In this way the first layer will be called the \textit{input layer}, the last layer the \textit{output layer} and all layers in between \textit{hidden layers} \citep[p. 227]{bishop-2006}. \\
In the case of neural networks, the nodes in the system are referred to as neurons.

\begin{Figure}
 \centering
 \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=0.4cm,y=0.4cm]
\clip(0,0) rectangle (15,15);
\draw [line width=1pt,fill=ffqqqq,fill opacity=1] (2,7) circle (0.4cm);
\draw [line width=1pt,fill=ffqqqq,fill opacity=1] (2,10) circle (0.4cm);
\draw [line width=1pt,fill=qqffqq,fill opacity=1] (7,4) circle (0.4cm);
\draw [line width=1pt,fill=qqffqq,fill opacity=1] (7,7) circle (0.4cm);
\draw [line width=1pt,fill=qqffqq,fill opacity=1] (7,10) circle (0.4cm);
\draw [line width=1pt,fill=qqffqq,fill opacity=1] (7,13) circle (0.4cm);
\draw [line width=1pt,fill=qqttcc,fill opacity=1] (12,10) circle (0.4cm);
\draw [line width=1pt,fill=qqttcc,fill opacity=1] (12,7) circle (0.4cm);
\draw [line width=1pt] (3,10)-- (6,13);
\draw [line width=1pt] (3,10)-- (6,10);
\draw [line width=1pt] (3,10)-- (6,7);
\draw [line width=1pt] (3,10)-- (6,4);
\draw [line width=1pt] (3,7)-- (6,13);
\draw [line width=1pt] (3,7)-- (6,10);
\draw [line width=1pt] (3,7)-- (6,7);
\draw [line width=1pt] (3,7)-- (6,4);
\draw [line width=1pt] (8,13)-- (11,10);
\draw [line width=1pt] (8,10)-- (11,10);
\draw [line width=1pt] (8,7)-- (11,10);
\draw [line width=1pt] (8,4)-- (11,10);
\draw [line width=1pt] (8,4)-- (11,7);
\draw [line width=1pt] (11,7)-- (8,7);
\draw [line width=1pt] (8,10)-- (11,7);
\draw [line width=1pt] (11,7)-- (8,13);
\end{tikzpicture}
 \captionsetup{width=0.8\linewidth, font=small}
 \captionof{figure}{An extremely simple neural network.}
\end{Figure}
\noindent The neural network shown consists of two input neurons (red), one hidden layer of three neurons (green) and two output neurons(blue). The black edges between the neurons represent the weights and biases of the system.\\
Thus, if one was to describe it mathematically, the value $z_i$ of neuron layer \textit{i} with \textit{d} number of neurons in it, given the matrix of sets of weights \textit{w} (the biases being the very first row) and the activation function $h()$ can be expressed as follows:
\[
z_i = h\left(\sum_{j=0}^d w_{ij}\cdot z_{i-1} + w_{i0}\right)
\]
Such a model is useful for calculating continuous values as well as for classification purposes. A common example of the latter is the MNIST data set of hand-written digits in 28x28 pixel greyscale images. In this case the light intensity of the 784 ($28^2$) pixels could fittingly be the input layer, while a layer consisting of 10 nodes (corresponding to digits 0 through 9) could be the output layer, such that the system could be used to predict which digit is written in a given image.
%\begin{figure}[h]
%  \centering
%  \frame{\includegraphics[width=0.7\linewidth]{images/mnist}}
%  \caption{Handwritten digits in the MNIST data set.}
%\end{figure}

\begin{Figure}
 \centering
 \includegraphics[width=0.7\linewidth]{images/mnist}
 \captionsetup{width=0.8\linewidth, font=small}
 \captionof{figure}{Handwritten digits in the MNIST data set.}
\end{Figure}

\noindent A common implementation of a neural network to perform this task is to have a single hidden layer of 800 nodes, however the most successful implementations in regards to the MNIST data set have been made with \textit{convolutional neural networks}, but we shall return to these later.

\subsubsection{Training}
Simply having defined the system of the neural network as above (denoted $f_{NN}$) naturally does not give a reliable method for classifying, as the accuracy of the prediction inherently depends on the correctness of the weights and biases of the system (denoted $\Theta$), which must be initialized with random values. \\
Enter a concept named \textit{backpropagation}. Having a set of predicted values ($\hat{y}$) for a data set (\textit{x}) along with the observed correct values (\textit{y}), one can establish a \textit{loss} by applying a loss function (that we shall elaborate on later), but which can be as straightforward as a root mean square error. This loss function will be denoted $l(\hat{y},y)$, so that the total loss of the system when applied to data set \textit{x} is $l(f_{NN}(\Theta, x), \hat{y})$. \citep[p. 241]{bishop-2006}\\
Backpropagation refers to the practice of essentially taking the derivative of the loss function in regards to the matrix of weights and biases, thereby calculating a \textit{gradient} $\nabla{l(f_{NN}(\Theta, x), \hat{y})}$ of these values with respect to the accuracy of the system, and then adjusting the weights and biases by the gradient multiplied by a learning rate scalar.\\
In layman's terms: by doing this one gets a matrix of "change your weights by this and that much in this and that direction to increase prediction accuracy".


\subsubsection{Convolutional neural networks}
Before introducing convolutional neural networks, a specification of what convolutions are is needed. The word convolution comes from the Latin convolvere, meaning to roll or to coil and still has a similar meaning in English today. In mathematics convolution is an operation where, for two functions $f(x)$ and $g(x)$, one is shifted across the other, resulting in a third function $h(x)$ that express the the intersection of the two functions. As such $h(x)$ explains how the shape of one of the functions is changed by the other. \citep[p. 267]{bishop-2006}
However the more practical explanation in the case of our thesis, is that it is the resulting matrix of the product of a matrix and a filter. For example if one has the matrix $a$ and the filter $f$:
\[
a = \begin{bmatrix}
       2 & 3 & 4           \\[0.3em]
       1 & 2 & 3 \\[0.3em]
       0 & 1 & 2
     \end{bmatrix}
\hspace*{1cm}
f = \begin{bmatrix}
       0 & 1 \\[0.3em]
       1 & 0        
     \end{bmatrix}     
\]

\noindent the resulting output of convolving $f$ over $a$ would be:

\[
\begin{bmatrix}
       4 & 6 \\[0.3em]
       2 & 4        
     \end{bmatrix}     
\]

where convolving works as follows:
\[
\begin{bmatrix}
       \textcolor{red}2 & \textcolor{red}3 & 4           \\[0.3em]
       \textcolor{red}1 & \textcolor{red}2 & 3 \\[0.3em]
       0 & 1 & 2
     \end{bmatrix}
\hspace*{0.5cm}
*
\hspace*{0.5cm}
\begin{bmatrix}
       0 & 1 \\[0.3em]
       1 & 0        
     \end{bmatrix}     
\hspace*{1cm}
 = \begin{bmatrix}
       \textcolor{red}4 &  \\[0.3em]
         &         
     \end{bmatrix}
\]

\[
\begin{bmatrix}
       2 & \textcolor{red}3 & \textcolor{red}4           \\[0.3em]
       1 & \textcolor{red}2 & \textcolor{red}3 \\[0.3em]
       0 & 1 & 2
     \end{bmatrix}
\hspace*{0.5cm}
*
\hspace*{0.5cm}
\begin{bmatrix}
       0 & 1 \\[0.3em]
       1 & 0        
     \end{bmatrix}     
\hspace*{1cm}
 = \begin{bmatrix}
       4 & \textcolor{red}6 \\[0.3em]
         &         
     \end{bmatrix}
\]
\[
\begin{bmatrix}
       2 & 3 & 4           \\[0.3em]
       \textcolor{red}1 & \textcolor{red}2 & 3 \\[0.3em]
       \textcolor{red}0 & \textcolor{red}1 & 2
     \end{bmatrix}
\hspace*{0.5cm}
*
\hspace*{0.5cm}
\begin{bmatrix}
       0 & 1 \\[0.3em]
       1 & 0        
     \end{bmatrix}     
\hspace*{1cm}
 = \begin{bmatrix}
       4 & 6 \\[0.3em]
       \textcolor{red}2 &         
     \end{bmatrix}
\]
\[
\begin{bmatrix}
       2 & 3 & 4           \\[0.3em]
       1 & \textcolor{red}2 & \textcolor{red}3 \\[0.3em]
       0 & \textcolor{red}1 & \textcolor{red}2
     \end{bmatrix}
\hspace*{0.5cm}
*
\hspace*{0.5cm}
\begin{bmatrix}
       0 & 1 \\[0.3em]
       1 & 0        
     \end{bmatrix}     
\hspace*{1cm}
 = \begin{bmatrix}
       4 & 6 \\[0.3em]
       2 & \textcolor{red}4        
     \end{bmatrix}
\]

As can be seen on the above example there are several things to consider in regard to convolution. First is, when done as above, the size of the input matrix is different than the output matrix due to the filter size. This filter is also known as the kernel. If it had been important to have an output matrix the same size as the input, a way to achieve this would be using padding. Padding refers to the practice of adding new data points around the original matrix, so that a filter can convolve over the data and preserve the size of the matrix. Selecting values to pad with can be done in several ways, however the most common way when constructing neural networks is zero-padding. \\
In the example, zero padding would be adding zeros to the input matrix so it became a $4\times4$ matrix, so when when shifting the kernel across, the output would be the original $3\times3$. \\
Of course the amount of padding needed also depends on the kernel size. Had the kernel been size $3\times3$, for the output matrix to be $3\times3$, it would have been necessary to pad zeros all around the original input matrix so it would be a $5\times5$ before convolution. \\
Another parameter worth considering when doing convolution is what is called stride. In the above example it was shown how convolution started in the upper left corner of the matrix, then moved the kernel one step to the right, then down to the left and then to the right again. This was a stride of size 1. If the matrix had been a $6\times6$, and the kernel still was a size $2\times2$, convolving would still have started in the upper left corner, then moved one step to the right, then another, and then another, until the right side of the kernel had shifted to align with the right side of the matrix. It would then be moved one step down and all the way to the left again, only to continue one step at a time to the right once more. \\
Had it been a stride of size 2 instead with a $6\times6$ input and a kernel of size $2\times2$, it would start in the upper left, then move 2 to the right instead of one, but otherwise follow the same pattern of movement. \citep[ch. 9]{goodfellow-et-al-2016} \\
Manipulating stride, kernel size and padding allows for controlling the size of the intermediary and final layers of convolutional networks. This is useful for making sure that the shape of the output of a model actually matches the subject the model is supposed to predict.

\subsubsection{Multi-task learning}
The foundation of multi-task learning is that when training a neural network to predict a certain task given a set of parameters, by training it to predict another or maybe several other tasks instead of a single one, each individual prediction will reach a better accuracy. The idea is to use domain-specific information in the training signals of related tasks to improve the generalization of a model. By training a model to have good predictions on more than one task, data dependent noise related to one task will then be identified easier as something to be ignored and thereby the general accuracy will increase. \citep{ruder-2017}\\
Furthermore when having a limited dataset with a lot of noise, the model will, by having several tasks to predict, be better at "focusing" attention on the relevant features that lead to correct predictions, while ignoring data specific features that might lead to overfitting. 

\subsection{Prior research in this field}

As protein secondary structure prediction is an important field for several scientific fields with ongoing research and real world application, it follows that there exists a large series of articles on the subject, four of which are used extensively in this paper. \\
The main article was \citeauthor{zhou-and-troyanskaya-2014}’s “Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure prediction” which provided both insight into and understanding of neural networks in relation to proteins, as well as serve as origin of the data used in this paper. \\
The next paper used was \citeauthor{qi-et-al-2012}’s “A unified Multitask Architecture for Predicting Local Protein Properties”. As Zhou and Troyanskaya used the method from this paper to discretize solvent accessibility scores to absolute and relative solvent accessibility, it brought to the table further understanding of the dataset. Furthermore the paper is about using multi-task learning for secondary structure prediction, and as the final goal of this thesis is to understand and implement a convolutional neural network using multi-task learning, a lot of inspiration was drawn from this paper. Lastly this paper gives a good understanding and basic introduction to the biological background of proteins and the features of the protein one can try to predict. 
The two last research papers are \citeauthor{wang-et-al-2016}’s “Protein Secondary Structure Prediction Using Deep Convolutional Neural Fields” and \citeauthor{ruder-2017}’s ”An Overview of Multi-Task Learning in Deep Neural Networks”.  As Wang et al. also predicted secondary structures using convolution we took inspiration from here as well, especially when optimizing our hyperparameters. Further, this article supplied an even better and more updated understanding of expectancy for accuracy as they achieve ~73 \% accuracy on a CullPDB dataset. \\
\cite{ruder-2017} being the last main paper, gives a good introduction to the methodology of multi-task learning and the ideas behind. As such much of the methodology of this paper concerning multi-task learning will come from his article.










